This project implements a RAG model using Langchain and Llama3.1. 
It can answer questions based off of context in the form of .md files from a vector database (Chroma). 
It can improve answers by opting to either use the given context or search on the web if the context fails to properly answer the question.
Note that the LLM (Llama 3.1) is hosted locally.
